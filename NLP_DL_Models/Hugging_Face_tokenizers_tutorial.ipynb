{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Tokebizers Tutorial and Usage\n",
    "\n",
    "Ref:- https://www.kaggle.com/funtowiczmo/hugging-face-tutorials-training-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tokenizers==0.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import NFKC, Lowercase, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\n",
    "# First we create an empty Byte Pair Encoding model\n",
    "tokenizer = Tokenizer(BPE.empty())\n",
    "\n",
    "# Lower casing and unicode-normalization\n",
    "tokenizer.normalizer = Sequence([NFKC(), Lowercase()])\n",
    "\n",
    "# use a pre-tokenizer convert a input to a Byte Level representation\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "\n",
    "# use a decoder so we can recover from a tokenized input to the original one\n",
    "tokenizer.decoder = ByteLevelDecoder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various options for the various stages\n",
    "We designed the library so that it provides all the required blocks to create end-to-end tokenizers in an interchangeable way. In that sense, we provide these various components:\n",
    "\n",
    "1. **Normalizer**: Executes all the initial transformations over the initial input string. For example when you need to lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer.\n",
    "2. **PreTokenizer**: In charge of splitting the initial input string. That's the component that decides where and how to pre-segment the origin string. The simplest example would be like we saw before, to simply split on spaces.\n",
    "3. **Model**: Handles all the sub-token discovery and generation, this part is trainable and really dependant of your input data.\n",
    "4. **Post-Processor**: Provides advanced construction features to be compatible with some of the Transformers-based SoTA models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.\n",
    "5. **Decoder**: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according to the PreTokenizer we used previously.\n",
    "6. **Trainer**: Provides training capabilities to each model.\n",
    "\n",
    "For each of the components above we provide multiple implementations:\n",
    "\n",
    "1. **Normalizer**: Lowercase, Unicode (NFD, NFKD, NFC, NFKC), Bert, Strip, ...\n",
    "2. **PreTokenizer**: ByteLevel, WhitespaceSplit, CharDelimiterSplit, Metaspace, ...\n",
    "3. **Model**: WordLevel, BPE, WordPiece\n",
    "4. **Post-Processor**: BertProcessor, ...\n",
    "5. **Decoder**: WordLevel, BPE, WordPiece, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained vocab size 25000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(vocab_size=25000, show_progress=True, initial_alphabet=ByteLevel.alphabet())\n",
    "\n",
    "tokenizer.train(trainer, [\"./big.txt\"])\n",
    "\n",
    "print(f\"Trained vocab size {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vocab.json', './merges.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model.save(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded String ['Ġu', '.', 's', '.', 'a', 'Ġis', 'Ġhandling', 'Ġthe', 'Ġk', 'ov', 'id', '19', 'Ġcrisis', 'Ġnot', 'Ġvery', 'Ġwell']\n",
      "Decoded String  u.s.a is handling the kovid19 crisis not very well\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "tokenizer.model = BPE.from_files(\"./vocab.json\", \"./merges.txt\")\n",
    "\n",
    "# encoding = tokenizer.encode(\"This is a simple input to be tokenized\")\n",
    "# encoding = tokenizer.encode(\"What is your name? My Name is Abhik\")\n",
    "encoding = tokenizer.encode(\"U.S.A is handling the kovid19 crisis not very well\")\n",
    "# encoding = tokenizer.encode(\"Banks are planning to cut the rates. The Federal govt. will reduce rates to 0.\")\n",
    "\n",
    "\n",
    "print(f\"Encoded String {encoding.tokens}\")\n",
    "\n",
    "decoded = tokenizer.decode(encoding.ids)\n",
    "\n",
    "print(f\"Decoded String {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other options\n",
    "The Encoding structure exposes multiple properties which are useful when working with transformers models\n",
    "\n",
    "1. **normalized_str**: The input string after normalization (lower-casing, unicode, stripping, etc.)\n",
    "2. **original_str**: The input string as it was provided\n",
    "3. **tokens**: The generated tokens with their string representation\n",
    "4. **input_ids**: The generated tokens with their integer representation\n",
    "5. **attention_mask**: If your input has been padded by the tokenizer, then this would be a vector of 1 for any non padded token and 0 for padded ones.\n",
    "6. **special_token_mask**: If your input contains special tokens such as [CLS], [SEP], [MASK], [PAD], then this would be a vector with 1 in places where a special token has been added.\n",
    "7. **type_ids**: If your was made of multiple \"parts\" such as (question, context), then this would be a vector with for each token the segment it belongs to.\n",
    "8. **overflowing**: If your has been truncated into multiple subparts because of a length limit (for BERT for example the sequence length is limited to 512), this will contain all the remaining overflowing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original String - U.S.A is handling the kovid19 crisis not very well\n",
      "Normalized String - u.s.a is handling the kovid19 crisis not very well\n",
      "String Tokens - ['Ġu', '.', 's', '.', 'a', 'Ġis', 'Ġhandling', 'Ġthe', 'Ġk', 'ov', 'id', '19', 'Ġcrisis', 'Ġnot', 'Ġvery', 'Ġwell']\n",
      " Attention mask - [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Special Token Mask - U.S.A is handling the kovid19 crisis not very well\n",
      "Type Ids - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Overflowing - []\n"
     ]
    }
   ],
   "source": [
    "# use some more functions that could also be used\n",
    "print(f\"Original String - {encoding.original_str}\")\n",
    "print(f\"Normalized String - {encoding.normalized_str}\")\n",
    "print(f\"String Tokens - {encoding.tokens}\")\n",
    "# print(f\"String Input Ids - {encoding.input_ids}\")\n",
    "print(f\" Attention mask - {encoding.attention_mask}\")\n",
    "print(f\"Special Token Mask - {encoding.original_str}\")\n",
    "print(f\"Type Ids - {encoding.type_ids}\")\n",
    "print(f\"Overflowing - {encoding.overflowing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
